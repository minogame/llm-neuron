import numpy as np
import matplotlib.pyplot as plt
from scipy import signal
import matplotlib
import sys, os
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
import oopsi

sentence = """
Inventors have long dreamed of creating machines that think. This desire datesback to at least the time of ancient Greece. The mythical ﬁgures Pygmalion,Daedalus, and Hephaestus may all be interpreted as legendary inventors, andGalatea, Talos, and Pandora may all be regarded as artiﬁcial life (Ovid and Martin,2004; Sparkes, 1996; Tandy, 1997).When programmable computers were ﬁrst conceived, people wondered whethersuch machines might become intelligent, over a hundred years before one wasbuilt (Lovelace, 1842). Today,artiﬁcial intelligence(AI) is a thriving ﬁeld withmany practical applications and active research topics. We look to intelligentsoftware to automate routine labor, understand speech or images, make diagnosesin medicine and support basic scientiﬁc research.In the early days of artiﬁcial intelligence, the ﬁeld rapidly tackled and solvedproblems that are intellectually diﬃcult for human beings but relatively straight-forward for computers—problems that can be described by a list of formal, math-ematical rules. The true challenge to artiﬁcial intelligence proved to be solvingthe tasks that are easy for people to perform but hard for people to describeformally—problems that we solve intuitively, that feel automatic, like recognizingspoken words or faces in images.This book is about a solution to these more intuitive problems. This solution isto allow computers to learn from experience and understand the world in terms ofa hierarchy of concepts, with each concept deﬁned through its relation to simplerconcepts. By gathering knowledge from experience, this approach avoids the needfor human operators to formally specify all the knowledge that the computer needs.The hierarchy of concepts enables the computer to learn complicated concepts bybuilding them out of simpler ones. If we draw a graph showing how these concepts
are built on top of each other, the graph is deep, with many layers. For this reason,we call this approach to AI deep learning.Many of the early successes of AI took place in relatively sterile and formalenvironments and did not require computers to have much knowledge aboutthe world. For example, IBM’s Deep Blue chess-playing system defeated worldchampion Garry Kasparov in 1997 (Hsu, 2002). Chess is of course a very simpleworld, containing only sixty-four locations and thirty-two pieces that can movein only rigidly circumscribed ways. Devising a successful chess strategy is atremendous accomplishment, but the challenge is not due to the diﬃculty ofdescribing the set of chess pieces and allowable moves to the computer. Chesscan be completely described by a very brief list of completely formal rules, easilyprovided ahead of time by the programmer.Ironically, abstract and formal tasks that are among the most diﬃcult mentalundertakings for a human being are among the easiest for a computer. Computershave long been able to defeat even the best human chess player but only recentlyhave begun matching some of the abilities of average human beings to recognizeobjects or speech. A person’s everyday life requires an immense amount ofknowledge about the world. Much of this knowledge is subjective and intuitive,and therefore diﬃcult to articulate in a formal way. Computers need to capturethis same knowledge in order to behave in an intelligent way. One of the keychallenges in artiﬁcial intelligence is how to get this informal knowledge into acomputer.Several artiﬁcial intelligence projects have sought to hard-code knowledgeabout the world in formal languages. A computer can reason automatically aboutstatements in these formal languages using logical inference rules. This is known astheknowledge baseapproach to artiﬁcial intelligence. None of these projects hasled to a major success. One of the most famous such projects is Cyc (Lenat andGuha, 1989). Cyc is an inference engine and a database of statements in a languagecalled CycL. These statements are entered by a staﬀ of human supervisors. It is anunwieldy process. People struggle to devise formal rules with enough complexityto accurately describe the world. For example, Cyc failed to understand a storyabout a person named Fred shaving in the morning (Linde, 1992). Its inferenceengine detected an inconsistency in the story: it knew that people do not haveelectrical parts, but because Fred was holding an electric razor, it believed theentity “FredWhileShaving” contained electrical parts. It therefore asked whetherFred was still a person while he was shaving.The diﬃculties faced by systems relying on hard-coded knowledge suggestthat AI systems need the ability to acquire their own knowledge, by extracting
patterns from raw data. This capability is known asmachine learning. Theintroduction of machine learning enabled computers to tackle problems involvingknowledge of the real world and make decisions that appear subjective. A simplemachine learning algorithm calledlogistic regressioncan determine whether torecommend cesarean delivery (Mor-Yosef et al., 1990). A simple machine learningalgorithm called naive Bayes can separate legitimate e-mail from spam e-mail.The performance of these simple machine learning algorithms depends heavilyon therepresentationof the data they are given. For example, when logisticregression is used to recommend cesarean delivery, the AI system does not examinethe patient directly. Instead, the doctor tells the system several pieces of relevantinformation, such as the presence or absence of a uterine scar. Each piece ofinformation included in the representation of the patient is known as afeature.Logistic regression learns how each of these features of the patient correlates withvarious outcomes. However, it cannot inﬂuence how features are deﬁned in anyway. If logistic regression were given an MRI scan of the patient, rather thanthe doctor’s formalized report, it would not be able to make useful predictions.Individual pixels in an MRI scan have negligible correlation with any complicationsthat might occur during delivery.
This dependence on representations is a general phenomenon that appearsthroughout computer science and even daily life. In computer science, operationssuch as searching a collection of data can proceed exponentially faster if the collec-tion is structured and indexed intelligently. People can easily perform arithmeticon Arabic numerals but ﬁnd arithmetic on Roman numerals much more timeconsuming. It is not surprising that the choice of representation has an enormouseﬀect on the performance of machine learning algorithms. For a simple visualexample, see ﬁgure 1.1.Many artiﬁcial intelligence tasks can be solved by designing the right set offeatures to extract for that task, then providing these features to a simple machinelearning algorithm. For example, a useful feature for speaker identiﬁcation fromsound is an estimate of the size of the speaker’s vocal tract. This feature gives astrong clue as to whether the speaker is a man, woman, or child.For many tasks, however, it is diﬃcult to know what features should beextracted. For example, suppose that we would like to write a program to detectcars in photographs. We know that cars have wheels, so we might like to use thepresence of a wheel as a feature. Unfortunately, it is diﬃcult to describe exactlywhat a wheel looks like in terms of pixel values. A wheel has a simple geometricshape, but its image may be complicated by shadows falling on the wheel, the sunglaring oﬀ the metal parts of the wheel, the fender of the car or an object in the
foreground obscuring part of the wheel, and so on.One solution to this problem is to use machine learning to discover not onlythe mapping from representation to output but also the representation itself.This approach is known asrepresentation learning. Learned representationsoften result in much better performance than can be obtained with hand-designedrepresentations. They also enable AI systems to rapidly adapt to new tasks, withminimal human intervention. A representation learning algorithm can discover agood set of features for a simple task in minutes, or for a complex task in hours tomonths. Manually designing features for a complex task requires a great deal ofhuman time and eﬀort; it can take decades for an entire community of researchers.The quintessential example of a representation learning algorithm is theau-toencoder. An autoencoder is the combination of anencoderfunction, whichconverts the input data into a diﬀerent representation, and adecoderfunction,which converts the new representation back into the original format. Autoencodersare trained to preserve as much information as possible when an input is runthrough the encoder and then the decoder, but they are also trained to make thenew representation have various nice properties. Diﬀerent kinds of autoencodersaim to achieve diﬀerent kinds of properties.When designing features or algorithms for learning features, our goal is usuallyto separate thefactors of variationthat explain the observed data. In this
context, we use the word “factors” simply to refer to separate sources of inﬂuence;the factors are usually not combined by multiplication. Such factors are often notquantities that are directly observed. Instead, they may exist as either unobservedobjects or unobserved forces in the physical world that aﬀect observable quantities.They may also exist as constructs in the human mind that provide useful simplifyingexplanations or inferred causes of the observed data. They can be thought of asconcepts or abstractions that help us make sense of the rich variability in the data.When analyzing a speech recording, the factors of variation include the speaker’sage, their sex, their accent and the words they are speaking. When analyzing animage of a car, the factors of variation include the position of the car, its color,and the angle and brightness of the sun.A major source of diﬃculty in many real-world artiﬁcial intelligence applicationsis that many of the factors of variation inﬂuence every single piece of data we areable to observe. The individual pixels in an image of a red car might be very closeto black at night. The shape of the car’s silhouette depends on the viewing angle.Most applications require us to disentangle the factors of variation and discard theones that we do not care about.Of course, it can be very diﬃcult to extract such high-level, abstract featuresfrom raw data. Many of these factors of variation, such as a speaker’s accent,can be identiﬁed only using sophisticated, nearly human-level understanding ofthe data. When it is nearly as diﬃcult to obtain a representation as to solve theoriginal problem, representation learning does not, at ﬁrst glance, seem to help us.Deep learningsolves this central problem in representation learning by intro-ducing representations that are expressed in terms of other, simpler representations.Deep learning enables the computer to build complex concepts out of simpler con-cepts. Figure 1.2 shows how a deep learning system can represent the concept ofan image of a person by combining simpler concepts, such as corners and contours,which are in turn deﬁned in terms of edges.The quintessential example of a deep learning model is the feedforward deepnetwork, ormultilayer perceptron(MLP). A multilayer perceptron is just amathematical function mapping some set of input values to output values. Thefunction is formed by composing many simpler functions. We can think of eachapplication of a diﬀerent mathematical function as providing a new representationof the input.The idea of learning the right representation for the data provides one per-spective on deep learning. Another perspective on deep learning is that depthenables the computer to learn a multistep computer program. Each layer of therepresentation can be thought of as the state of the computer’s memory after
executing another set of instructions in parallel. Networks with greater depth canexecute more instructions in sequence. Sequential instructions oﬀer great powerbecause later instructions can refer back to the results of earlier instructions. Ac-cording to this view of deep learning, not all the information in a layer’s activationsnecessarily encodes factors of variation that explain the input. The representationalso stores state information that helps to execute a program that can make senseof the input. This state information could be analogous to a counter or pointerin a traditional computer program. It has nothing to do with the content of theinput speciﬁcally, but it helps the model to organize its processing.There are two main ways of measuring the depth of a model. The ﬁrst view isbased on the number of sequential instructions that must be executed to evaluatethe architecture. We can think of this as the length of the longest path througha ﬂow chart that describes how to compute each of the model’s outputs givenits inputs. Just as two equivalent computer programs will have diﬀerent lengthsdepending on which language the program is written in, the same function maybe drawn as a ﬂowchart with diﬀerent depths depending on which functions weallow to be used as individual steps in the ﬂowchart. Figure 1.3 illustrates how thischoice of language can give two diﬀerent measurements for the same architecture.
Another approach, used by deep probabilistic models, regards the depth of amodel as being not the depth of the computational graph but the depth of thegraph describing how concepts are related to each other. In this case, the depthof the ﬂowchart of the computations needed to compute the representation ofeach concept may be much deeper than the graph of the concepts themselves.This is because the system’s understanding of the simpler concepts can be reﬁnedgiven information about the more complex concepts. For example, an AI systemobserving an image of a face with one eye in shadow may initially see only oneeye. After detecting that a face is present, the system can then infer that a secondeye is probably present as well. In this case, the graph of concepts includes onlytwo layers—a layer for eyes and a layer for faces—but the graph of computationsincludes 2nlayers if we reﬁne our estimate of each concept given the otherntimes.Because it is not always clear which of these two views—the depth of thecomputational graph, or the depth of the probabilistic modeling graph—is mostrelevant, and because diﬀerent people choose diﬀerent sets of smallest elementsfrom which to construct their graphs, there is no single correct value for thedepth of an architecture, just as there is no single correct value for the length ofa computer program. Nor is there a consensus about how much depth a modelrequires to qualify as “deep.” However, deep learning can be safely regarded as thestudy of models that involve a greater amount of composition of either learnedfunctions or learned concepts than traditional machine learning does.To summarize, deep learning, the subject of this book, is an approach to AI.Speciﬁcally, it is a type of machine learning, a technique that enables computersystems to improve with experience and data. We contend that machine learningis the only viable approach to building AI systems that can operate in complicatedreal-world environments. Deep learning is a particular kind of machine learningthat achieves great power and ﬂexibility by representing the world as a nestedhierarchy of concepts, with each concept deﬁned in relation to simpler concepts, andmore abstract representations computed in terms of less abstract ones. Figure 1.4illustrates the relationship between these diﬀerent AI disciplines. Figure 1.5 givesa high-level schematic of how each works.
1.1 Who Should Read This Book?This book can be useful for a variety of readers, but we wrote it with two targetaudiences in mind. One of these target audiences is university students (under-graduate or graduate) learning about machine learning, including those who arebeginning a career in deep learning and artiﬁcial intelligence research. The other
target audience is software engineers who do not have a machine learning or statis-tics background but want to rapidly acquire one and begin using deep learning intheir product or platform. Deep learning has already proved useful in many soft-ware disciplines, including computer vision, speech and audio processing, naturallanguage processing, robotics, bioinformatics and chemistry, video games, searchengines, online advertising and ﬁnance.This book has been organized into three parts to best accommodate a varietyof readers. Part I introduces basic mathematical tools and machine learningconcepts. Part II describes the most established deep learning algorithms, whichare essentially solved technologies. Part III describes more speculative ideas thatare widely believed to be important for future research in deep learning.
Readers should feel free to skip parts that are not relevant given their interestsor background. Readers familiar with linear algebra, probability, and fundamentalmachine learning concepts can skip part I, for example, while those who just wantto implement a working system need not read beyond part II. To help choose which
chapters to read, ﬁgure 1.6 provides a ﬂowchart showing the high-level organizationof the book.We do assume that all readers come from a computer science background. Weassume familiarity with programming, a basic understanding of computationalperformance issues, complexity theory, introductory level calculus and some of theterminology of graph theory.1.2 Historical Trends in Deep LearningIt is easiest to understand deep learning with some historical context. Rather thanproviding a detailed history of deep learning, we identify a few key trends:•Deep learning has had a long and rich history, but has gone by many names,reﬂecting diﬀerent philosophical viewpoints, and has waxed and waned inpopularity.•Deep learning has become more useful as the amount of available trainingdata has increased.•Deep learning models have grown in size over time as computer infrastructure(both hardware and software) for deep learning has improved.•Deep learning has solved increasingly complicated applications with increasingaccuracy over time.1.2.1 The Many Names and Changing Fortunes of Neural Net-worksWe expect that many readers of this book have heard of deep learning as an excitingnew technology, and are surprised to see a mention of “history” in a book about anemerging ﬁeld. In fact, deep learning dates back to the 1940s. Deep learning onlyappears to be new, because it was relatively unpopular for several years precedingits current popularity, and because it has gone through many diﬀerent names, onlyrecently being called “deep learning.” The ﬁeld has been rebranded many times,reﬂecting the inﬂuence of diﬀerent researchers and diﬀerent perspectives.A comprehensive history of deep learning is beyond the scope of this textbook.Some basic context, however, is useful for understanding deep learning. Broadlyspeaking, there have been three waves of development: deep learning known ascyberneticsin the 1940s–1960s, deep learning known asconnectionismin the
1980s–1990s, and the current resurgence under the name deep learning beginningin 2006. This is quantitatively illustrated in ﬁgure 1.7.Some of the earliest learning algorithms we recognize today were intended tobe computational models of biological learning, that is, models of how learninghappens or could happen in the brain. As a result, one of the names that deeplearning has gone by isartiﬁcial neural networks(ANNs). The correspondingperspective on deep learning models is that they are engineered systems inspiredby the biological brain (whether the human brain or the brain of another animal).While the kinds of neural networks used for machine learning have sometimesbeen used to understand brain function (Hinton and Shallice, 1991), they aregenerally not designed to be realistic models of biological function. The neuralperspective on deep learning is motivated by two main ideas. One idea is thatthe brain provides a proof by example that intelligent behavior is possible, and aconceptually straightforward path to building intelligence is to reverse engineer thecomputational principles behind the brain and duplicate its functionality. Another
CHAPTER 1. INTRODUCTIONperspective is that it would be deeply interesting to understand the brain and theprinciples that underlie human intelligence, so machine learning models that shedlight on these basic scientiﬁc questions are useful apart from their ability to solveengineering applications.The modern term “deep learning” goes beyond the neuroscientiﬁc perspectiveon the current breed of machine learning models. It appeals to a more generalprinciple of learning multiple levels of composition, which can be applied in machinelearning frameworks that are not necessarily neurally inspired.The earliest predecessors of modern deep learning were simple linear modelsmotivated from a neuroscientiﬁc perspective. These models were designed totake a set ofninput valuesx1, . . . , xnand associate them with an outputy.These models would learn a set of weightsw1, . . . , wnand compute their outputf(x, w) =x1w1+···+xnwn. This ﬁrst wave of neural networks research wasknown as cybernetics, as illustrated in ﬁgure 1.7.The McCulloch-Pitts neuron (McCulloch and Pitts, 1943) was an early modelof brain function. This linear model could recognize two diﬀerent categories ofinputs by testing whetherf(x, w) is positive or negative. Of course, for the modelto correspond to the desired deﬁnition of the categories, the weights needed to beset correctly. These weights could be set by the human operator. In the 1950s, theperceptron (Rosenblatt, 1958, 1962) became the ﬁrst model that could learn theweights that deﬁned the categories given examples of inputs from each category.Theadaptive linear element(ADALINE), which dates from about the sametime, simply returned the value off(x) itself to predict a real number (Widrowand Hoﬀ, 1960) and could also learn to predict these numbers from data.These simple learning algorithms greatly aﬀected the modern landscape of ma-chine learning. The training algorithm used to adapt the weights of the ADALINEwas a special case of an algorithm calledstochastic gradient descent. Slightlymodiﬁed versions of the stochastic gradient descent algorithm remain the dominanttraining algorithms for deep learning models today.Models based on thef(x, w) used by the perceptron and ADALINE are calledlinear models. These models remain some of the most widely used machinelearning models, though in many cases they are trained in diﬀerent ways than theoriginal models were trained.Linear models have many limitations. Most famously, they cannot learn theXOR function, wheref([0,1], w) = 1 andf([1,0], w) = 1 butf([1,1], w) = 0andf([0,0], w) = 0. Critics who observed these ﬂaws in linear models causeda backlash against biologically inspired learning in general (Minsky and Papert,1969). This was the ﬁrst major dip in the popularity of neural networks.
CHAPTER 1. INTRODUCTIONToday, neuroscience is regarded as an important source of inspiration for deeplearning researchers, but it is no longer the predominant guide for the ﬁeld.The main reason for the diminished role of neuroscience in deep learningresearch today is that we simply do not have enough information about the brainto use it as a guide. To obtain a deep understanding of the actual algorithms usedby the brain, we would need to be able to monitor the activity of (at the veryleast) thousands of interconnected neurons simultaneously. Because we are notable to do this, we are far from understanding even some of the most simple andwell-studied parts of the brain (Olshausen and Field, 2005).Neuroscience has given us a reason to hope that a single deep learning algorithmcan solve many diﬀerent tasks. Neuroscientists have found that ferrets can learn to“see” with the auditory processing region of their brain if their brains are rewiredto send visual signals to that area (Von Melchner et al., 2000). This suggests thatmuch of the mammalian brain might use a single algorithm to solve most of thediﬀerent tasks that the brain solves. Before this hypothesis, machine learningresearch was more fragmented, with diﬀerent communities of researchers studyingnatural language processing, vision, motion planning and speech recognition. Today,these application communities are still separate, but it is common for deep learningresearch groups to study many or even all these application areas simultaneously.We are able to draw some rough guidelines from neuroscience. The basicidea of having many computational units that become intelligent only via theirinteractions with each other is inspired by the brain. The neocognitron (Fukushima,1980) introduced a powerful model architecture for processing images that wasinspired by the structure of the mammalian visual system and later became thebasis for the modern convolutional network (LeCun et al., 1998b), as we will seein section 9.10. Most neural networks today are based on a model neuron calledtherectiﬁed linear unit. The original cognitron (Fukushima, 1975) introduceda more complicated version that was highly inspired by our knowledge of brainfunction. The simpliﬁed modern version was developed incorporating ideas frommany viewpoints, with Nair and Hinton (2010) and Glorot et al. (2011a) citingneuroscience as an inﬂuence, and Jarrett et al. (2009) citing more engineering-oriented inﬂuences. While neuroscience is an important source of inspiration, itneed not be taken as a rigid guide. We know that actual neurons compute verydiﬀerent functions than modern rectiﬁed linear units, but greater neural realismhas not yet led to an improvement in machine learning performance. Also, whileneuroscience has successfully inspired several neural network architectures, wedo not yet know enough about biological learning for neuroscience to oﬀer muchguidance for the learning algorithms we use to train these architectures.
"""

# --- 1. 模拟数据 (请用您的真实数据替换这部分) ---
# 假设:
N_TRIALS = 29       # 试验总数
N_NEURONS = 1024    # 您的神经元数量
N_TIMEPOINTS = 4096 # 每个试验的时间点
SAMPLING_RATE = 1 # 假设的采样率 (Hz) - !! 这对PSD计算至关重要

np_load = np.load("outcomes/runb_hidden_states.npy")  # shape: (N_TRIALS, N_TIMEPOINTS, N_NEURONS)
list_of_trials = [np_load[t, 0] for t in range(N_TRIALS)]  # 用真实数据替换

F_signal = list_of_trials[-2][:, 0]  # 取第一个trial的第一个神经元信号作为示例

T = N_TIMEPOINTS
dt = 0.020
lam = 0.1
tau = 1.5
sigma = 0.2

# fix random seed
np.random.seed(42)

# --- 2. 运行 Py-OOPSI (Fast-OOPSI 算法) ---
# F_signal 是我们的输入数据
# 我们需要提供时间步长 dt
# tau_est 是算法的初始猜测值，这里我们假设知道大概范围
print("正在运行 Py-OOPSI 算法...")

try:
    # fast() 函数会返回推断的 spikes
    # 注意：根据您安装的 py-oopsi 版本 (原版或 simple-oopsi fork)
    # API 可能略有不同。这是 'simple-oopsi' (pip install py-oopsi 常见的安装) 的 API
    inferred_spikes, _ = oopsi.fast(F_signal, dt=dt)
    inferred_spikes = np.asarray(inferred_spikes, dtype=float)
    print("Spike 推断完成。")

    print(f"inferred_spikes shape: {inferred_spikes.shape}")
    print(f"inferred_spikes (first 20 values): {inferred_spikes[:20]}")

except Exception as e:
    print(f"Py-OOPSI 运行出错: {e}")
    print("请检查 'py-oopsi' 库是否已正确安装。")
    inferred_spikes = np.zeros_like(F_signal) # 出错时返回空值


# --- 3. 可视化结果 (Visualize Results) ---

plt.figure(figsize=(18, 7))

# (a) 绘制荧光信号
plt.plot(F_signal, 'b', label='Noisy Fluorescence (F)', alpha=0.6)

# # (b) 绘制真实的 Spikes (Ground Truth)
# #     为了看清楚，我们用 stem 图，并放大 spike
# (true_markers, true_stems, _) = plt.stem(
#     np.where(true_spikes > 0.5)[0],  # Spike 的时间点
#     true_spikes[true_spikes > 0.5] * np.max(F_signal) * 0.8, # Spike 的高度
#     linefmt='g-', markerfmt='go', basefmt=' ',
#     label='Ground Truth Spikes'
# )
# plt.setp(true_stems, 'linewidth', 1.5)

# (c) 绘制推断的 Spikes (Inferred Spikes)
#     py-oopsi 的输出是 spike 的“概率”或“强度”，不是严格的 0/1
#     我们同样用 stem 图来表示
(inferred_markers, inferred_stems, _) = plt.stem(
    np.arange(T), # 所有时间点
    inferred_spikes * np.max(F_signal) * 0.7, # 缩放高度以便对比
    linefmt='r-', markerfmt='ro', basefmt=' ',
    label='Inferred Spikes (Py-OOPSI)'
)
plt.setp(inferred_stems, 'linewidth', 1, 'alpha', 0.7)
plt.setp(inferred_markers, 'markersize', 4, 'alpha', 0.7)


plt.legend(loc='upper right')
plt.xlabel(f'Time Points (dt={dt}s)')
plt.ylabel('Fluorescence / Spike Strength')
plt.title('Py-OOPSI (fast-oopsi) Spike Inference')
plt.savefig('outcomes/py_oopsi_spike_inference.png', dpi=300)