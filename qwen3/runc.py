import os
os.environ['CUDA_VISIBLE_DEVICES'] = '1'
os.environ["TRANSFORMERS_NO_TF"] = "1"
os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"
os.environ["USE_TF"] = "0"

from transformers import AutoTokenizer
from modeling_qwen3 import Qwen3ForCausalLM, Qwen3ForNeuronSignal
import sys, torch
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
from prepare_data import DataProcessor
from draw import DrawNeuron, DrawFFT
import numpy as np
from sklearn.metrics import r2_score

model_name = "Qwen/Qwen3-0.6B"

# load the tokenizer and the model
tokenizer = AutoTokenizer.from_pretrained(model_name)

model = Qwen3ForNeuronSignal.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)

# model = Qwen3ForCausalLM.from_pretrained(
#     model_name,
#     torch_dtype="auto",
#     device_map="auto"
# )

# # sentence = (
# #     "<think>"
# #     "Okay, the user wants a short introduction to a large language model. Let me start by recalling what I know about LLMs. They're big language models, right? So I should mention their ability to understand and generate text. Maybe start with the basics: how they work. Then explain their capabilities, like answering questions, creating content, etc. Also, highlight their advantages over traditional models. Oh, and maybe touch on their applications in various fields. Keep it concise but informative. Make sure it's clear and easy to understand. Avoid technical jargon. Let me structure it in a way that flows well from introduction to main points."
# #     "</think>"
# #     "A large language model (LLM) is a type of artificial intelligence system designed to understand and generate human language. These models are trained on vast datasets to comprehend complex texts, answer questions, and create creative content. They are capable of tasks like writing essays, generating stories, or even translating languages. Unlike traditional models, LLMs can process and respond to a wide range of inputs, making them versatile and powerful for various applications in fields like healthcare, education, and beyond."
# #     "<|im_end|>"
# # )

# sentence = """
# Inventors have long dreamed of creating machines that think. This desire datesback to at least the time of ancient Greece. The mythical ﬁgures Pygmalion,Daedalus, and Hephaestus may all be interpreted as legendary inventors, andGalatea, Talos, and Pandora may all be regarded as artiﬁcial life (Ovid and Martin,2004; Sparkes, 1996; Tandy, 1997).When programmable computers were ﬁrst conceived, people wondered whethersuch machines might become intelligent, over a hundred years before one wasbuilt (Lovelace, 1842). Today,artiﬁcial intelligence(AI) is a thriving ﬁeld withmany practical applications and active research topics. We look to intelligentsoftware to automate routine labor, understand speech or images, make diagnosesin medicine and support basic scientiﬁc research.In the early days of artiﬁcial intelligence, the ﬁeld rapidly tackled and solvedproblems that are intellectually diﬃcult for human beings but relatively straight-forward for computers—problems that can be described by a list of formal, math-ematical rules. The true challenge to artiﬁcial intelligence proved to be solvingthe tasks that are easy for people to perform but hard for people to describeformally—problems that we solve intuitively, that feel automatic, like recognizingspoken words or faces in images.This book is about a solution to these more intuitive problems. This solution isto allow computers to learn from experience and understand the world in terms ofa hierarchy of concepts, with each concept deﬁned through its relation to simplerconcepts. By gathering knowledge from experience, this approach avoids the needfor human operators to formally specify all the knowledge that the computer needs.The hierarchy of concepts enables the computer to learn complicated concepts bybuilding them out of simpler ones. If we draw a graph showing how these concepts
# are built on top of each other, the graph is deep, with many layers. For this reason,we call this approach to AI deep learning.Many of the early successes of AI took place in relatively sterile and formalenvironments and did not require computers to have much knowledge aboutthe world. For example, IBM’s Deep Blue chess-playing system defeated worldchampion Garry Kasparov in 1997 (Hsu, 2002). Chess is of course a very simpleworld, containing only sixty-four locations and thirty-two pieces that can movein only rigidly circumscribed ways. Devising a successful chess strategy is atremendous accomplishment, but the challenge is not due to the diﬃculty ofdescribing the set of chess pieces and allowable moves to the computer. Chesscan be completely described by a very brief list of completely formal rules, easilyprovided ahead of time by the programmer.Ironically, abstract and formal tasks that are among the most diﬃcult mentalundertakings for a human being are among the easiest for a computer. Computershave long been able to defeat even the best human chess player but only recentlyhave begun matching some of the abilities of average human beings to recognizeobjects or speech. A person’s everyday life requires an immense amount ofknowledge about the world. Much of this knowledge is subjective and intuitive,and therefore diﬃcult to articulate in a formal way. Computers need to capturethis same knowledge in order to behave in an intelligent way. One of the keychallenges in artiﬁcial intelligence is how to get this informal knowledge into acomputer.Several artiﬁcial intelligence projects have sought to hard-code knowledgeabout the world in formal languages. A computer can reason automatically aboutstatements in these formal languages using logical inference rules. This is known astheknowledge baseapproach to artiﬁcial intelligence. None of these projects hasled to a major success. One of the most famous such projects is Cyc (Lenat andGuha, 1989). Cyc is an inference engine and a database of statements in a languagecalled CycL. These statements are entered by a staﬀ of human supervisors. It is anunwieldy process. People struggle to devise formal rules with enough complexityto accurately describe the world. For example, Cyc failed to understand a storyabout a person named Fred shaving in the morning (Linde, 1992). Its inferenceengine detected an inconsistency in the story: it knew that people do not haveelectrical parts, but because Fred was holding an electric razor, it believed theentity “FredWhileShaving” contained electrical parts. It therefore asked whetherFred was still a person while he was shaving.The diﬃculties faced by systems relying on hard-coded knowledge suggestthat AI systems need the ability to acquire their own knowledge, by extracting
# patterns from raw data. This capability is known asmachine learning. Theintroduction of machine learning enabled computers to tackle problems involvingknowledge of the real world and make decisions that appear subjective. A simplemachine learning algorithm calledlogistic regressioncan determine whether torecommend cesarean delivery (Mor-Yosef et al., 1990). A simple machine learningalgorithm called naive Bayes can separate legitimate e-mail from spam e-mail.The performance of these simple machine learning algorithms depends heavilyon therepresentationof the data they are given. For example, when logisticregression is used to recommend cesarean delivery, the AI system does not examinethe patient directly. Instead, the doctor tells the system several pieces of relevantinformation, such as the presence or absence of a uterine scar. Each piece ofinformation included in the representation of the patient is known as afeature.Logistic regression learns how each of these features of the patient correlates withvarious outcomes. However, it cannot inﬂuence how features are deﬁned in anyway. If logistic regression were given an MRI scan of the patient, rather thanthe doctor’s formalized report, it would not be able to make useful predictions.Individual pixels in an MRI scan have negligible correlation with any complicationsthat might occur during delivery.
# This dependence on representations is a general phenomenon that appearsthroughout computer science and even daily life. In computer science, operationssuch as searching a collection of data can proceed exponentially faster if the collec-tion is structured and indexed intelligently. People can easily perform arithmeticon Arabic numerals but ﬁnd arithmetic on Roman numerals much more timeconsuming. It is not surprising that the choice of representation has an enormouseﬀect on the performance of machine learning algorithms. For a simple visualexample, see ﬁgure 1.1.Many artiﬁcial intelligence tasks can be solved by designing the right set offeatures to extract for that task, then providing these features to a simple machinelearning algorithm. For example, a useful feature for speaker identiﬁcation fromsound is an estimate of the size of the speaker’s vocal tract. This feature gives astrong clue as to whether the speaker is a man, woman, or child.For many tasks, however, it is diﬃcult to know what features should beextracted. For example, suppose that we would like to write a program to detectcars in photographs. We know that cars have wheels, so we might like to use thepresence of a wheel as a feature. Unfortunately, it is diﬃcult to describe exactlywhat a wheel looks like in terms of pixel values. A wheel has a simple geometricshape, but its image may be complicated by shadows falling on the wheel, the sunglaring oﬀ the metal parts of the wheel, the fender of the car or an object in the
# foreground obscuring part of the wheel, and so on.One solution to this problem is to use machine learning to discover not onlythe mapping from representation to output but also the representation itself.This approach is known asrepresentation learning. Learned representationsoften result in much better performance than can be obtained with hand-designedrepresentations. They also enable AI systems to rapidly adapt to new tasks, withminimal human intervention. A representation learning algorithm can discover agood set of features for a simple task in minutes, or for a complex task in hours tomonths. Manually designing features for a complex task requires a great deal ofhuman time and eﬀort; it can take decades for an entire community of researchers.The quintessential example of a representation learning algorithm is theau-toencoder. An autoencoder is the combination of anencoderfunction, whichconverts the input data into a diﬀerent representation, and adecoderfunction,which converts the new representation back into the original format. Autoencodersare trained to preserve as much information as possible when an input is runthrough the encoder and then the decoder, but they are also trained to make thenew representation have various nice properties. Diﬀerent kinds of autoencodersaim to achieve diﬀerent kinds of properties.When designing features or algorithms for learning features, our goal is usuallyto separate thefactors of variationthat explain the observed data. In this
# context, we use the word “factors” simply to refer to separate sources of inﬂuence;the factors are usually not combined by multiplication. Such factors are often notquantities that are directly observed. Instead, they may exist as either unobservedobjects or unobserved forces in the physical world that aﬀect observable quantities.They may also exist as constructs in the human mind that provide useful simplifyingexplanations or inferred causes of the observed data. They can be thought of asconcepts or abstractions that help us make sense of the rich variability in the data.When analyzing a speech recording, the factors of variation include the speaker’sage, their sex, their accent and the words they are speaking. When analyzing animage of a car, the factors of variation include the position of the car, its color,and the angle and brightness of the sun.A major source of diﬃculty in many real-world artiﬁcial intelligence applicationsis that many of the factors of variation inﬂuence every single piece of data we areable to observe. The individual pixels in an image of a red car might be very closeto black at night. The shape of the car’s silhouette depends on the viewing angle.Most applications require us to disentangle the factors of variation and discard theones that we do not care about.Of course, it can be very diﬃcult to extract such high-level, abstract featuresfrom raw data. Many of these factors of variation, such as a speaker’s accent,can be identiﬁed only using sophisticated, nearly human-level understanding ofthe data. When it is nearly as diﬃcult to obtain a representation as to solve theoriginal problem, representation learning does not, at ﬁrst glance, seem to help us.Deep learningsolves this central problem in representation learning by intro-ducing representations that are expressed in terms of other, simpler representations.Deep learning enables the computer to build complex concepts out of simpler con-cepts. Figure 1.2 shows how a deep learning system can represent the concept ofan image of a person by combining simpler concepts, such as corners and contours,which are in turn deﬁned in terms of edges.The quintessential example of a deep learning model is the feedforward deepnetwork, ormultilayer perceptron(MLP). A multilayer perceptron is just amathematical function mapping some set of input values to output values. Thefunction is formed by composing many simpler functions. We can think of eachapplication of a diﬀerent mathematical function as providing a new representationof the input.The idea of learning the right representation for the data provides one per-spective on deep learning. Another perspective on deep learning is that depthenables the computer to learn a multistep computer program. Each layer of therepresentation can be thought of as the state of the computer’s memory after
# executing another set of instructions in parallel. Networks with greater depth canexecute more instructions in sequence. Sequential instructions oﬀer great powerbecause later instructions can refer back to the results of earlier instructions. Ac-cording to this view of deep learning, not all the information in a layer’s activationsnecessarily encodes factors of variation that explain the input. The representationalso stores state information that helps to execute a program that can make senseof the input. This state information could be analogous to a counter or pointerin a traditional computer program. It has nothing to do with the content of theinput speciﬁcally, but it helps the model to organize its processing.There are two main ways of measuring the depth of a model. The ﬁrst view isbased on the number of sequential instructions that must be executed to evaluatethe architecture. We can think of this as the length of the longest path througha ﬂow chart that describes how to compute each of the model’s outputs givenits inputs. Just as two equivalent computer programs will have diﬀerent lengthsdepending on which language the program is written in, the same function maybe drawn as a ﬂowchart with diﬀerent depths depending on which functions weallow to be used as individual steps in the ﬂowchart. Figure 1.3 illustrates how thischoice of language can give two diﬀerent measurements for the same architecture.
# Another approach, used by deep probabilistic models, regards the depth of amodel as being not the depth of the computational graph but the depth of thegraph describing how concepts are related to each other. In this case, the depthof the ﬂowchart of the computations needed to compute the representation ofeach concept may be much deeper than the graph of the concepts themselves.This is because the system’s understanding of the simpler concepts can be reﬁnedgiven information about the more complex concepts. For example, an AI systemobserving an image of a face with one eye in shadow may initially see only oneeye. After detecting that a face is present, the system can then infer that a secondeye is probably present as well. In this case, the graph of concepts includes onlytwo layers—a layer for eyes and a layer for faces—but the graph of computationsincludes 2nlayers if we reﬁne our estimate of each concept given the otherntimes.Because it is not always clear which of these two views—the depth of thecomputational graph, or the depth of the probabilistic modeling graph—is mostrelevant, and because diﬀerent people choose diﬀerent sets of smallest elementsfrom which to construct their graphs, there is no single correct value for thedepth of an architecture, just as there is no single correct value for the length ofa computer program. Nor is there a consensus about how much depth a modelrequires to qualify as “deep.” However, deep learning can be safely regarded as thestudy of models that involve a greater amount of composition of either learnedfunctions or learned concepts than traditional machine learning does.To summarize, deep learning, the subject of this book, is an approach to AI.Speciﬁcally, it is a type of machine learning, a technique that enables computersystems to improve with experience and data. We contend that machine learningis the only viable approach to building AI systems that can operate in complicatedreal-world environments. Deep learning is a particular kind of machine learningthat achieves great power and ﬂexibility by representing the world as a nestedhierarchy of concepts, with each concept deﬁned in relation to simpler concepts, andmore abstract representations computed in terms of less abstract ones. Figure 1.4illustrates the relationship between these diﬀerent AI disciplines. Figure 1.5 givesa high-level schematic of how each works.
# 1.1 Who Should Read This Book?This book can be useful for a variety of readers, but we wrote it with two targetaudiences in mind. One of these target audiences is university students (under-graduate or graduate) learning about machine learning, including those who arebeginning a career in deep learning and artiﬁcial intelligence research. The other
# target audience is software engineers who do not have a machine learning or statis-tics background but want to rapidly acquire one and begin using deep learning intheir product or platform. Deep learning has already proved useful in many soft-ware disciplines, including computer vision, speech and audio processing, naturallanguage processing, robotics, bioinformatics and chemistry, video games, searchengines, online advertising and ﬁnance.This book has been organized into three parts to best accommodate a varietyof readers. Part I introduces basic mathematical tools and machine learningconcepts. Part II describes the most established deep learning algorithms, whichare essentially solved technologies. Part III describes more speculative ideas thatare widely believed to be important for future research in deep learning.
# Readers should feel free to skip parts that are not relevant given their interestsor background. Readers familiar with linear algebra, probability, and fundamentalmachine learning concepts can skip part I, for example, while those who just wantto implement a working system need not read beyond part II. To help choose which
# chapters to read, ﬁgure 1.6 provides a ﬂowchart showing the high-level organizationof the book.We do assume that all readers come from a computer science background. Weassume familiarity with programming, a basic understanding of computationalperformance issues, complexity theory, introductory level calculus and some of theterminology of graph theory.1.2 Historical Trends in Deep LearningIt is easiest to understand deep learning with some historical context. Rather thanproviding a detailed history of deep learning, we identify a few key trends:•Deep learning has had a long and rich history, but has gone by many names,reﬂecting diﬀerent philosophical viewpoints, and has waxed and waned inpopularity.•Deep learning has become more useful as the amount of available trainingdata has increased.•Deep learning models have grown in size over time as computer infrastructure(both hardware and software) for deep learning has improved.•Deep learning has solved increasingly complicated applications with increasingaccuracy over time.1.2.1 The Many Names and Changing Fortunes of Neural Net-worksWe expect that many readers of this book have heard of deep learning as an excitingnew technology, and are surprised to see a mention of “history” in a book about anemerging ﬁeld. In fact, deep learning dates back to the 1940s. Deep learning onlyappears to be new, because it was relatively unpopular for several years precedingits current popularity, and because it has gone through many diﬀerent names, onlyrecently being called “deep learning.” The ﬁeld has been rebranded many times,reﬂecting the inﬂuence of diﬀerent researchers and diﬀerent perspectives.A comprehensive history of deep learning is beyond the scope of this textbook.Some basic context, however, is useful for understanding deep learning. Broadlyspeaking, there have been three waves of development: deep learning known ascyberneticsin the 1940s–1960s, deep learning known asconnectionismin the
# 1980s–1990s, and the current resurgence under the name deep learning beginningin 2006. This is quantitatively illustrated in ﬁgure 1.7.Some of the earliest learning algorithms we recognize today were intended tobe computational models of biological learning, that is, models of how learninghappens or could happen in the brain. As a result, one of the names that deeplearning has gone by isartiﬁcial neural networks(ANNs). The correspondingperspective on deep learning models is that they are engineered systems inspiredby the biological brain (whether the human brain or the brain of another animal).While the kinds of neural networks used for machine learning have sometimesbeen used to understand brain function (Hinton and Shallice, 1991), they aregenerally not designed to be realistic models of biological function. The neuralperspective on deep learning is motivated by two main ideas. One idea is thatthe brain provides a proof by example that intelligent behavior is possible, and aconceptually straightforward path to building intelligence is to reverse engineer thecomputational principles behind the brain and duplicate its functionality. Another
# CHAPTER 1. INTRODUCTIONperspective is that it would be deeply interesting to understand the brain and theprinciples that underlie human intelligence, so machine learning models that shedlight on these basic scientiﬁc questions are useful apart from their ability to solveengineering applications.The modern term “deep learning” goes beyond the neuroscientiﬁc perspectiveon the current breed of machine learning models. It appeals to a more generalprinciple of learning multiple levels of composition, which can be applied in machinelearning frameworks that are not necessarily neurally inspired.The earliest predecessors of modern deep learning were simple linear modelsmotivated from a neuroscientiﬁc perspective. These models were designed totake a set ofninput valuesx1, . . . , xnand associate them with an outputy.These models would learn a set of weightsw1, . . . , wnand compute their outputf(x, w) =x1w1+···+xnwn. This ﬁrst wave of neural networks research wasknown as cybernetics, as illustrated in ﬁgure 1.7.The McCulloch-Pitts neuron (McCulloch and Pitts, 1943) was an early modelof brain function. This linear model could recognize two diﬀerent categories ofinputs by testing whetherf(x, w) is positive or negative. Of course, for the modelto correspond to the desired deﬁnition of the categories, the weights needed to beset correctly. These weights could be set by the human operator. In the 1950s, theperceptron (Rosenblatt, 1958, 1962) became the ﬁrst model that could learn theweights that deﬁned the categories given examples of inputs from each category.Theadaptive linear element(ADALINE), which dates from about the sametime, simply returned the value off(x) itself to predict a real number (Widrowand Hoﬀ, 1960) and could also learn to predict these numbers from data.These simple learning algorithms greatly aﬀected the modern landscape of ma-chine learning. The training algorithm used to adapt the weights of the ADALINEwas a special case of an algorithm calledstochastic gradient descent. Slightlymodiﬁed versions of the stochastic gradient descent algorithm remain the dominanttraining algorithms for deep learning models today.Models based on thef(x, w) used by the perceptron and ADALINE are calledlinear models. These models remain some of the most widely used machinelearning models, though in many cases they are trained in diﬀerent ways than theoriginal models were trained.Linear models have many limitations. Most famously, they cannot learn theXOR function, wheref([0,1], w) = 1 andf([1,0], w) = 1 butf([1,1], w) = 0andf([0,0], w) = 0. Critics who observed these ﬂaws in linear models causeda backlash against biologically inspired learning in general (Minsky and Papert,1969). This was the ﬁrst major dip in the popularity of neural networks.
# CHAPTER 1. INTRODUCTIONToday, neuroscience is regarded as an important source of inspiration for deeplearning researchers, but it is no longer the predominant guide for the ﬁeld.The main reason for the diminished role of neuroscience in deep learningresearch today is that we simply do not have enough information about the brainto use it as a guide. To obtain a deep understanding of the actual algorithms usedby the brain, we would need to be able to monitor the activity of (at the veryleast) thousands of interconnected neurons simultaneously. Because we are notable to do this, we are far from understanding even some of the most simple andwell-studied parts of the brain (Olshausen and Field, 2005).Neuroscience has given us a reason to hope that a single deep learning algorithmcan solve many diﬀerent tasks. Neuroscientists have found that ferrets can learn to“see” with the auditory processing region of their brain if their brains are rewiredto send visual signals to that area (Von Melchner et al., 2000). This suggests thatmuch of the mammalian brain might use a single algorithm to solve most of thediﬀerent tasks that the brain solves. Before this hypothesis, machine learningresearch was more fragmented, with diﬀerent communities of researchers studyingnatural language processing, vision, motion planning and speech recognition. Today,these application communities are still separate, but it is common for deep learningresearch groups to study many or even all these application areas simultaneously.We are able to draw some rough guidelines from neuroscience. The basicidea of having many computational units that become intelligent only via theirinteractions with each other is inspired by the brain. The neocognitron (Fukushima,1980) introduced a powerful model architecture for processing images that wasinspired by the structure of the mammalian visual system and later became thebasis for the modern convolutional network (LeCun et al., 1998b), as we will seein section 9.10. Most neural networks today are based on a model neuron calledtherectiﬁed linear unit. The original cognitron (Fukushima, 1975) introduceda more complicated version that was highly inspired by our knowledge of brainfunction. The simpliﬁed modern version was developed incorporating ideas frommany viewpoints, with Nair and Hinton (2010) and Glorot et al. (2011a) citingneuroscience as an inﬂuence, and Jarrett et al. (2009) citing more engineering-oriented inﬂuences. While neuroscience is an important source of inspiration, itneed not be taken as a rigid guide. We know that actual neurons compute verydiﬀerent functions than modern rectiﬁed linear units, but greater neural realismhas not yet led to an improvement in machine learning performance. Also, whileneuroscience has successfully inspired several neural network architectures, wedo not yet know enough about biological learning for neuroscience to oﬀer muchguidance for the learning algorithms we use to train these architectures.
# """

sentence = """
Emergent Scale-Free Dynamics: Exploring the Ability of Large Language Models to Predict Biological Neural Time Series
Abstract
Although Large Language Models (LLMs) are increasingly used as models for cognitive processes, their potential for predicting raw, non-linguistic neural dynamics remains underexplored. This study demonstrates that a general-purpose, pre-trained Large Language Model can predict mouse brain fluorescence signals with an accuracy comparable to a bespoke Transformer model pre-trained on synthetic pink noise. To explain this unexpected efficacy, we analyzed the LLM's internal representations and found that its hidden state activations exhibit emergent scale-free ($1/f$) dynamics, a statistical hallmark consistent with the neural signals themselves. We conclude that the powerful inductive bias LLMs acquire from pre-training on natural language manifests as an intrinsic dynamical structure aligned with the fundamental statistical properties of complex biological systems, thereby enabling effective cross-domain prediction. This finding suggests a deeper, shared principle of information processing between advanced artificial and biological neural networks.
1. Introduction
1.1 The Challenge of Modeling Neural Dynamics
Predicting the future evolution of neural activity is a fundamental goal in neuroscience, crucial for understanding brain function, developing brain-computer interfaces, and diagnosing neurological disorders. However, neural signals are inherently highly complex, non-linear, and stochastic [1, 2]. These signals reflect the intricate interactions of millions or billions of neurons, with dynamics spanning multiple temporal and spatial scales. Consequently, constructing computational models that can accurately capture and predict this complex behavior has remained a significant challenge for the field.
1.2 Transformers as a New Paradigm for Time-Series Forecasting
In recent years, the advent of the Transformer architecture has revolutionized sequential data modeling [3]. Unlike Recurrent Neural Networks (RNNs) which rely on recurrent connections, Transformers are based entirely on attention mechanisms, enabling them to capture long-range dependencies in sequences [1]. This property has led to immense success in Natural Language Processing (NLP) and has been rapidly adopted for various time-series forecasting tasks, including finance and biological signals [4]. Despite the strong performance of Transformer-based models in time-series forecasting, how to effectively adapt them to the unique statistical properties of different domains remains an active area of research [4].
1.3 An Unconventional Hypothesis: Leveraging LLMs for Neural Signal Prediction
Large Language Models (LLMs) are the product of large-scale pre-training of Transformer architectures on massive text corpora. There is emerging research exploring the use of LLMs to decode cognitive states or interpret EEG signals in clinical contexts [5]. However, applying them directly to the prediction of continuous, raw neural signals represents a significant conceptual leap. The core question of this paper is: Could a model trained on the statistical properties of human language possess the internal mechanisms to simulate the dynamics of a biological brain? This question is partly inspired by the conceptual similarity between the LLM's core objective (next-token prediction) and the theory of predictive coding in the brain, both of which rely on predicting future inputs based on context [6, 7].
1.4 The Explanatory Bridge: 1/f Noise in Natural and Artificial Systems
Pink noise, or $1/f$ noise, is a ubiquitous signal characteristic found in complex, self-organizing systems. Its power spectral density (PSD) follows a power-law distribution, $S(f) \propto 1/f^\alpha$, where the exponent $\alpha$ for frequency $f$ is approximately 1 [8, 9]. This statistical property indicates a system balanced between predictability and randomness and possessing long-range temporal correlations [10, 11].
In biology, $1/f$ noise is a universal phenomenon, especially in neural activity. It is found in signals ranging from single-neuron firing to large-scale EEG and fMRI, and is often considered a hallmark of a healthy, efficient information processing system operating at a state of criticality [2, 9, 12, 13]. Crucially, recent studies have found that when artificial neural networks (like LSTMs, and as this study will show, Transformers) are trained on natural data (like language), their internal activations spontaneously develop similar $1/f$ noise characteristics [14, 15]. This suggests that $1/f$ dynamics may be a fundamental property that emerges in any sufficiently complex network as it learns to process correlated data.
1.5 Hypothesis and Paper Structure
Based on this background, we propose our central hypothesis: The unexpected success of a pre-trained LLM in predicting neural time series is not coincidental. It stems from an emergent alignment between its internal statistical dynamics and those of the neural signal. Specifically, we hypothesize that the LLM's internal representations have self-organized to exhibit $1/f$ noise properties, thereby mirroring the intrinsic statistical nature of the brain signals themselves. This paper will first demonstrate the LLM's predictive performance against a baseline model that explicitly accounts for these statistics, and then delve into an analysis of the LLM's internal states to validate our hypothesis. This research framework proposes a new form of inductive bias: dynamical inductive bias. Whereas traditional inductive biases are often structural (e.g., the locality of CNNs), we argue that the LLM's pre-training not only teaches it the syntax and semantics of language but, more importantly, forces its internal state transitions to adhere to the statistical laws (like long-range correlations and $1/f$ dynamics) common to natural signals. It is this dynamical bias that allows the model to effectively zero-shot transfer to another domain, like neuroscience, that shares these "natural" dynamical features.
2. Models and Experimental Design
This section details the dataset, model architectures, task setup, and analysis methods used in the experiment to ensure the reproducibility of the study.
2.1 Neural Time Series Dataset
The data used in this study were derived from in vivo two-photon calcium imaging of the awake mouse brain. Specifically, we recorded neuronal population activity in a specific cortical region using GCaMP6f as the calcium indicator. During data acquisition, the mice were engaged in a specific behavioral task. The raw image data underwent a rigorous preprocessing pipeline, including motion correction, automatic segmentation of Regions of Interest (ROIs), extraction of the raw fluorescence trace for each ROI, and conversion to relative fluorescence change (ΔF/F). Finally, we obtained a set of one-dimensional time-series vectors, each representing the activity of a single neuron or a local neuronal population.
2.2 Prediction Task Formulation
We formulate the prediction task as an autoregressive problem: using past signal values to predict future signal values.
Input and Output: For each time point, the model receives a historical sequence of length $L_{in}$ and is required to predict the future sequence of length $L_{out}$.
Data Normalization: All time-series data were normalized using a z-score (mean=0, std=1) before being fed into the models.
Input Representation: As Transformer models are originally designed to process discrete tokens, we must map the continuous fluorescence signal values into the model's embedding space. We employ a trainable linear projection layer to map the continuous input vector of length $L_{in}$ directly into a sequence of embedding vectors of dimension $d_{model}$.
2.3 Model Architectures and Implementation
2.3.1 Large Language Model Forecaster (LLM Forecaster)
Model Choice: We selected a publicly available pre-trained LLM, such as a medium-sized model from the GPT-2 series.
Architectural Parameters: Following standard reporting practices [3, 16], we specify the model's key parameters: number of layers $N$, model/embedding dimension $d_{model}$, number of attention heads $h$, and feed-forward network internal dimension $d_{ff}$.
Adaptation for Forecasting: To adapt the LLM for the forecasting task, we removed its original language model head (typically a linear layer for predicting vocabulary probabilities). In its place, we added a new regression head on top of the final Transformer layer's output. This regression head is a simple linear layer that projects the $d_{model}$-dimensional hidden state vector to an $L_{out}$-dimensional output, corresponding to the predicted future time series.
Training Strategy: During training, the LLM's core pre-trained weights were frozen. Only the newly added input projection layer and output regression head were trained. This strategy ensures we are evaluating the knowledge inherent in the pre-trained model, rather than new knowledge learned through extensive fine-tuning.
2.3.2 Pink-Noise Pre-trained Transformer Baseline (PN-Former)
Architecture: For a fair comparison, the PN-Former adopted the exact same Transformer architecture parameters as the LLM ($N, d_{model}, h, d_{ff}$), but its weights were randomly initialized.
Pre-training Data: We synthesized a large corpus of pink noise time series as pre-training data. These sequences were generated using the standard Voss–McCartney algorithm [9], with their power spectral density strictly following the $S(f) \propto 1/f$ distribution.
Pre-training Objective: The PN-Former was pre-trained on a self-supervised task known as masked signal modeling. In this task, portions of the input sequence time points were randomly masked, and the model's objective was to reconstruct the values of these masked points based on the unmasked context.
Fine-tuning: After pre-training, the PN-Former model was fine-tuned on the real neural data using the same regression head and training protocol as the LLM Forecaster.
"""

sentence = """
  Non eram nescius, Brute, cum, quae summis ingeniis exquisitaque doctrina philosophi Graeco sermone tractavissent, ea Latinis litteris mandaremus, fore ut hic noster labor in varias reprehensiones incurreret; nam quibusdam, et iis quidem non admodum indoctis, totum hoc displicet philosophari. Quidam autem non tam id reprehendunt, si remissius agatur, sed tantum studium tamque multam operam ponendam in eo non arbitrantur. Erunt etiam, et hi quidem eruditi Graecis litteris, contemnentes Latinas, qui se dicant in Graecis legendis operam malle consumere. Postremo aliquos futuros suspicor, qui me ad alias litteras vocent, genus hoc scribendi, etsi sit elegans, personae tamen et dignitatis esse negent.   Contra quos omnes dicendum breviter existimo: quamquam philosophiae quidem vituperatoribus satis responsum est eo libro, quo a nobis philosophia defensa et collaudata est, cum esset accusata et vituperata ab Hortensio. Qui liber cum et tibi probatus videretur et iis, quos ego posse iudicare arbitrarer, plura suscepi, veritus ne movere hominum studia viderer, retinere non posse. Qui autem, si maxime hoc placeat, moderatius tamen id volunt fieri, difficilem quandam temperantiam postulant in eo, quod semel admissum coerceri reprimique non potest, ut propemodum iustioribus utamur illis, qui omnino avocent a philosophia, quam his, qui rebus infinitis modum constituant in reque eo meliore, quo maior sit, mediocritatem desiderent.   Sive enim ad sapientiam perveniri potest, non paranda nobis solum ea, sed fruenda etiam est; sive hoc difficile est, tamen nec modus est ullus investigandi veri, nisi inveneris, et quaerendi defetigatio turpis est, cum id, quod quaeritur, sit pulcherrimum. Etenim si delectamur, cum scribimus, quis est tam invidus, qui ab eo nos abducat? sin laboramus, quis est, qui alienae modum statuat industriae? Nam ut Terentianus Chremes non inhumanus, qui novum vicinum non vult

fodere aút arare aut áliquid ferre dénique,
— non enim illum ab industria, sed ab illiberali labore deterret,— sic isti curiosi, quos offendit noster minime nobis iniucundus labor.
  Iis igitur est difficilius satis facere, qui se Latina scripta dicunt contemnere. In quibus hoc primum est, in quo admirer, cur in gravissimis rebus non delectet eos sermo patrius, cum idem fabellas Latinas ad verbum e Graecis expressas non inviti legant. Quis enim tam inimicus paene nomini Romano est, qui Ennii Medeam aut Antiopam Pacuvii spernat aut reiciat, quod se iisdem Euripidis fabulis delectari dicat [Latinas litteras oderit]? Synephebos ego, inquit, potius Caecilii aut Andriam Terentii quam utramque Menandri legam?   A quibus tantum dissentio, ut, cum Sophocles vel optime scripserit Electram, tamen male conversam Atilii mihi legendam putem, de quo Licinius, ‘ferreum scriptorem’, verum, opinor, scriptorem tamen, ut legendus sit. Rudem enim esse omnino in nostris poetis aut inertissimae segnitiae est aut fastidii delicatissimi. Mihi quidem nulli satis eruditi videntur, quibus nostra ignota sunt. An

Utinám ne in némore
nihilo minus legimus quam hoc idem Graecum: quae autem de bene beateque vivendo a Platone disputata sunt, haec explicari non placebit Latine?   Quod si nos non interpretum fungimur munere, sed tuemur ea, quae dicta sunt ab iis, quos probamus, eisque nostrum iudicium et nostrum scribendi ordinem adiungimus, quid habent cur Graeca anteponant iis, quae et splendide dicta neque sint conversa de Graecis? Nam si dicent ab illis has res esse tractatas, ne ipsos quidem Graecos est cur tam multos legant, quam legendi sunt. Quid enim est a Chrysippo praetermissum in Stoicis? Legimus tamen Diogenem, Antipatrum, Mnesarchum, Panaetium, multos alios in primisque familiarem nostrum Posidonium. Quid? Theophrastus mediocriterne delectat, cum tractat locos ab Aristotele ante tractatos? quid? Epicurei num desistunt de iisdem, de quibus et ab Epicuro scriptum est et ab antiquis, ad arbitrium suum scribere? Quod si Graeci leguntur a Graecis iisdem de rebus alia ratione compositis, quid est cur nostri a nostris non legantur?
  Quamquam, si plane sic verterem Platonem aut Aristotelem, ut verterunt nostri poetae fabulas, male, credo, mererer de meis civibus, si ad eorum cognitionem divina illa ingenia transferrem: sed id neque feci adhuc nec mihi tamen ne faciam interdictum puto. Locos quidem quosdam, si videbitur, transferam, et maxime ab iis, quos modo nominavi, cum inciderit ut id apte fieri possit, ut ab Homero Ennius, Afranius a Menandro solet. Nec vero, ut noster Lucilius, recusabo quo minus omnes mea legant. Utinam esset ille Persius! Scipio vero et Rutilius multo etiam magis: quorum ille iudicium reformidans Tarentinis ait se et Consentinis et Siculis scribere. Facete is quidem, sicut alia; sed neque tam docti tum erant, ad quorum iudicium elaboraret, et sunt illius scripta leviora, ut urbanitas summa appareat, doctrina mediocris.   Ego autem quem timeam lectorem, cum ad te, ne Graecis quidem cedentem in philosophia, audeam scribere? quamquam a te ipso id quidem facio provocatus gratissimo mihi libro, quem ad me de virtute misisti. Sed ex eo credo quibusdam usu venire ut abhorreant a Latinis, quod inciderint in inculta quaedam et horrida, de malis Graecis Latine scripta deterius. Quibus ego assentior, dum modo de iisdem rebus ne Graecos quidem legendos putent. Res vero bonas, verbis electis graviter ornateque dictas, quis non legat? nisi qui se plane Graecum dici velit, ut a Scaevola est praetore salutatus Athenis Albucius.   Quem quidem locum cum multa venustate et omni sale idem Lucilius, apud quem praeclare Scaevola:

Graecum te, Albuci, quam Romanum atque Sabinum,
Municipem Ponti, Tritani, centurionum,
Praeclarorum hominum ac primorum signiferumque,
Maluisti dici. Graece ergo praetor Athenis,
Id quod maluisti, te, cum ad me accedi’, saluto:
Χαῖρε, inquam, Tite: lictores, turma omni’ cohorsque.
Χαῖρε, Tite! hinc hostis mi Albucius, hinc inimicus.

  Sed iure Mucius. Ego autem mirari satis non queo, unde hoc sit tam insolens domesticarum rerum fastidium. Non est omnino hic docendi locus, sed ita sentio et saepe disserui, Latinam linguam non modo non inopem, ut vulgo putarent, sed locupletiorem etiam esse quam Graecam. Quando enim nobis, vel dicam aut oratoribus bonis aut poetis, postea quidem quam fuit quem imitarentur, ullus orationis vel copiosae vel elegantis ornatus defuit?
Ego vero, quoniam forensibus operis, laboribus, periculis non deseruisse mihi videor praesidium, in quo a populo Romano locatus sum, debeo profecto, quantumcumque possum, in eo quoque elaborare, ut sint opera, studio, labore meo doctiores cives mei, nec cum istis tanto opere pugnare, qui Graeca legere malint, modo legant illa ipsa, ne simulent, et iis servire, qui vel utrisque litteris uti velint vel, si suas habent, illas non magno opere desiderent.   Qui autem alia malunt scribi a nobis, aequi esse debent, quod et scripta multa sunt, sic ut plura nemini e nostris, et scribentur fortasse plura, si vita suppetet: et tamen qui diligenter haec, quae de philosophia litteris mandamus, legere assueverit, iudicabit nulla ad legendum his esse potiora. Quid est enim in vita tanto opere quaerendum, quam cum omnia in philosophia, tum id, quod his libris quaeritur, qui sit finis, quid extremum, quid ultimum, quo sint omnia bene vivendi recteque faciendi consilia referenda, quid sequatur natura ut summum ex rebus expetendis, quid fugiat ut extremum malorum? Qua de re cum sit inter doctissimos summa dissensio, quis alienum putet eius esse dignitatis, quam mihi quisque tribuat, quid in omni munere vitae optimum et verissimum sit, exquirere?   An, partus ancillae sitne in fructu habendus, disseretur inter principes civitatis, P. Scaevolam Maniumque Manilium, ab iisque M. Brutus dissentiet, quod et acutum genus est et ad usus civium non inutile nosque ea scripta reliquaque eiusdem generis et legimus libenter et legemus: haec, quae vitam omnem continent, neglegentur? nam, ut sint illa vendibiliora, haec uberiora certe sunt. Quamquam id quidem licebit iis existimare, qui legerint.

Nos autem hanc omnem quaestionem de finibus bonorum et malorum fere a nobis explicatam esse his litteris arbitramur, in quibus, quantum potuimus, non modo quid nobis probaretur, sed etiam quid a singulis philosophiae disciplinis diceretur, persecuti sumus.
  Ut autem a facillimis ordiamur, prima veniat in medium Epicuri ratio, quae plerisque notissima est, quam a nobis sic intelleges expositam, ut ab ipsis, qui eam disciplinam probant, non soleat accuratius explicari. Verum enim invenire volumus, non tamquam adversarium aliquem convincere. Accurate autem quondam a L. Torquato, homine omni doctrina erudito, defensa est Epicuri sententia de voluptate, a meque ei responsum, cum C. Triarius, in primis gravis et doctus adulescens, ei disputationi interesset.   Nam cum ad me in Cumanum salutandi causa uterque venisset, pauca primo inter nos de litteris, quarum summum erat in utroque studium; deinde Torquatus, Quoniam nacti te, inquit, sumus aliquando otiosum, certe audiam, quid sit quod Epicurum nostrum non tu quidem oderis, ut fere faciunt qui ab eo dissentiunt, sed certe non probes, eum, quem ego arbitror unum vidisse verum maximisque erroribus animos hominum liberavisse et omnia tradidisse, quae pertinerent ad bene beateque vivendum: sed existimo te, sicut nostrum Triarium, minus ab eo delectari, quod ista Platonis, Aristoteli, Theophrasti orationis ornamenta neglexerit; nam illud quidem adduci vix possum, ut ea, quae senserit ille, tibi non vera videantur.   Vide quantum, inquam, fallare, Torquate. Oratio me istius philosophi non offendit; nam et complecitur verbis quod vult et dicit plane quod intellegam: et tamen ego a philosopho, si afferat eloquentiam, non asperner, si non habeat, non admodum flagitem: re mihi non aeque satis facit, et quidem locis pluribus. Sed quot homines, tot sententiae; falli igitur possumus. Quam ob rem tandem, inquit, non satis facit? te enim iudicem aequum puto, modo quae dicat ille bene noris.   Nisi mihi Phaedrum, inquam, mentitum aut Zenonem putas, quorum utrumque audivi, cum mihi nihil sane praeter sedulitatem probarent, omnes mihi Epicuri sententiae satis notae sunt, atque eos, quos nominavi, cum Attico nostro frequenter audivi, cum miraretur ille quidem utrumque, Phaedrum autem etiam amaret, cotidieque inter nos ea, quae audiebamus, conferebamus, neque erat unquam controversia, quid ego intellegerem, sed quid probarem.
"""

sentence = """
I was not unaware, Brutus, when I undertook to commit to Latin writing those subjects which philosophers of the highest genius and exquisite learning had treated in the Greek language, that this labor of mine would encounter various criticisms. For to some, and indeed men not entirely unlearned, this whole business of philosophizing is displeasing. Others, however, do not so much criticize it, if it is pursued more casually, but they do not think that so much enthusiasm and so much work should be spent on it. There will also be others, learned in Greek literature but contemptuous of Latin, who will say they prefer to spend their effort in reading Greek works. Finally, I suspect there will be some who call me to other kinds of literature, arguing that this type of writing, even if it is elegant, is nevertheless unbefitting my public position and dignity.

Against all of these, I think I must speak briefly. Although, indeed, a sufficient response has already been given to the critics of philosophy in that book in which philosophy was defended and praised by me, when it had been accused and criticized by Hortensius. Since that book seemed to be approved both by you and by those whom I judged capable of judging, I have undertaken more, fearing lest I might seem to have been able to arouse men's interest, but unable to sustain it. But as for those who, even if they approve of this activity, nevertheless want it done more moderately, they demand a difficult sort of moderation in a thing which, once admitted, cannot be checked or repressed; so that we might find those who call us away from philosophy entirely to be almost more reasonable than these men, who set a limit on infinite things and demand mediocrity in a subject that is better the greater it is.

For if wisdom can be attained, it must not only be acquired by us, but also enjoyed. Or if this is difficult, there is nevertheless no limit to seeking the truth, until you have found it; and weariness in the search is shameful, when that which is sought is the most beautiful thing of all. Indeed, if we take pleasure in writing, who is so envious as to draw us away from it? But if it is a labor, who is there to set a limit to another's industry? For just as Terence's Chremes is not unkind, who does not want his new neighbor "to dig or plow or, in short, to do any heavy lifting" —for he is not deterring him from industry, but from servile labor—so it is with these busybodies, who are offended by this labor of mine, which is not at all unpleasant to me.

It is more difficult, therefore, to satisfy those who say they despise Latin writings. Regarding them, this is the first thing at which I wonder: why their native tongue does not delight them in the most serious matters, when these same people willingly read Latin plays translated word-for-word from the Greek. For who is so hostile to the very name of Roman that he would spurn or reject the Medea of Ennius or the Antiopa of Pacuvius, because he says he delights in the same plays by Euripides [and hates Latin literature]? "Shall I," he says, "read Caecilius's Synephebi or Terence's Andria rather than Menander's version of both?"

I dissent from these people so much that, although Sophocles wrote an excellent Electra, I still think Atilius's poorly translated version is worth reading. Licinius called him "an iron writer"—a harsh one, I suppose, but a writer nonetheless, so he should be read. For to be completely unacquainted with our own poets is a mark of either the laziest indolence or the most delicate fastidiousness. To me, indeed, no one seems sufficiently learned to whom our own [literature] is unknown. Or do we read "Would that in the grove..." any less than the same line in Greek? But will it be displeasing for those things discussed by Plato about living well and happily to be explained in Latin?

But if we are not acting as mere translators, but are upholding those things said by those we approve of, and adding to them our own judgment and our own order of writing, what reason do they have to prefer the Greek works to these, which are both eloquently stated and are not [mere] translations from the Greek? For if they say that these subjects have already been treated by them [the Greeks], there is no reason for them to read even so many Greeks as they are accustomed to read. For what in Stoicism was overlooked by Chrysippus? Yet we read Diogenes, Antipater, Mnesarchus, Panaetius, many others, and especially our friend Posidonius. What? Does Theophrastus delight us only moderately when he treats topics previously treated by Aristotle? What? Do the Epicureans stop writing, according to their own judgment, about the same things which were written about by Epicurus and by the ancients? But if Greeks are read by Greeks on the same subjects composed in a different manner, why should our [authors] not be read by our [people]?

And yet, if I were to translate Plato or Aristotle just as our poets translated plays, I would, I believe, be doing a poor service to my fellow citizens, if I were to transfer those divine geniuses to their understanding [in that way]. But I have neither done this so far, nor do I think I am forbidden from doing it. Certain passages, indeed, I will translate, if it seems appropriate, and especially from those I just named, when it happens that it can be done fittingly, just as Ennius is accustomed [to do] from Homer, or Afranius from Menander. Nor indeed, like our Lucilius, will I object to everyone reading my work. Would that Persius were [alive]! Or Scipio and Rutilius even more so! Lucilius, fearing their judgment, says that he writes for the people of Tarentum, Consentia, and Sicily. He said this wittily, as with other things; but neither were there such learned men then whose judgment he had to labor for, and his writings are rather light, so that while his wit is supreme, his learning is moderate.

But what reader should I fear, when I dare to write to you, a man not inferior even to the Greeks in philosophy? Although I do this having been challenged by you yourself, in that most welcome book you sent me "On Virtue." But I believe it happens to some that they shrink from Latin works for this reason: because they have come across certain unpolished and rough works, written in Latin as bad translations from bad Greek. I agree with them, provided that they think the Greeks are not worth reading on these same subjects either. But as for good subjects, expressed gravely and elegantly in choice words, who would not read them? Unless he plainly wishes to be called a Greek, as Albucius was greeted in Athens by the praetor Scaevola.

The same Lucilius [describes] this incident with much charm and wit, in whose work Scaevola [speaks] brilliantly:

"You, Albucius, preferred to be called a Greek Rather than a Roman and a Sabine, A fellow citizen of Pontus, of Tritanus, of centurions, Illustrious men and standard-bearers. Therefore, as praetor in Athens, I salute you in Greek, As you preferred, when you approach me: 'Χαῖρε (Hail),' I say, 'Titus!' My lictors, my whole entourage, and my cohort [shout]: 'Χαῖρε (Hail), Titus!' From that moment, Albucius is my foe, my enemy."

But Mucius [Scaevola] was right. I, however, cannot wonder enough whence this insolent disdain for our native things arises. This is not the place to elaborate, but I feel and have often argued that the Latin language is not only not impoverished, as is commonly thought, but is even richer than Greek. For when have we—or I should say, our good orators or poets, at least after they had someone to imitate—lacked any ornament of speech, whether copious or elegant?

As for me, since I do not seem to have deserted my post in the forum—in its works, labors, and perils—in which I was placed by the Roman people, I certainly ought also to labor, as much as I can, to this end: that my fellow citizens may be more learned through my work, enthusiasm, and labor. And I should not fight so hard with those who prefer to read Greek, provided they actually read those works and don't just pretend to; and I should serve those who wish to use both literatures, or who, if they have their own, do not greatly miss the other.

But those who prefer that other things be written by me ought to be fair, because many things have already been written (so many that no one of our countrymen has written more), and perhaps more will be written, if life allows. And yet, anyone who makes a habit of diligently reading these things which I am committing to writing about philosophy will judge that nothing is more worth reading than these. For what in life is so worth seeking as all things in philosophy, and especially that which is sought in these books: what is the end, what is the extreme, what is the ultimate goal, to which all plans for living well and acting rightly should be referred? What does nature follow as the highest of things to be sought, and what does it flee from as the ultimate of evils? Since there is the greatest disagreement on this matter among the most learned men, who would think it foreign to whatever dignity one might attribute to me, to inquire what is best and truest in every duty of life?

Or, shall it be debated among the leaders of the state, P. Scaevola and Manius Manilius, whether the offspring of a female slave is to be considered profit, and will Marcus Brutus [your ancestor] disagree with them—because this is both a sharp-witted debate and not useless for the citizens' needs, and we read and will read those writings and others of the same kind with pleasure—and yet these matters, which contain all of life, will be neglected? For, though those [legal debates] may be more popular, these [philosophical ones] are certainly richer. Although, those who have read them will be free to judge this.

We, however, believe that this whole question of the ends of good and evil has been almost completely explained by us in these writings, in which, as much as we were able, we have pursued not only what was approved by us, but also what was said by each of the philosophical schools.

But to begin with the easiest, let the system of Epicurus come first, which is the most familiar to the majority. You will understand it as set forth by us in such a way that it is not usually explained more accurately even by those who approve of that school. For we wish to find the truth, not to refute some adversary. Moreover, the opinion of Epicurus concerning pleasure was once accurately defended by Lucius Torquatus, a man educated in every doctrine, and a response was given to him by me, while Gaius Triarius, a particularly serious and learned young man, was present at that discussion.

For when both of them had come to my estate at Cumae to pay their respects, at first we spoke a few words among ourselves about literature, for which both had the greatest enthusiasm. Then Torquatus said, "Since we have found you at leisure at last, I must certainly hear why it is that you—not that you hate our Epicurus, as those who disagree with him usually do—but certainly why you do not approve of him; the one man whom I believe alone saw the truth, liberated the minds of men from the greatest errors, and handed down all things that pertain to living well and happily. But I suppose that you, just like our friend Triarius, are less delighted by him because he neglected those rhetorical ornaments of Plato, Aristotle, and Theophrastus. For I can hardly be brought to believe that what he thought does not seem true to you."

"See how mistaken you are, Torquatus," I said. "The style of that philosopher does not offend me; for he both encompasses in words what he means and speaks plainly what I can understand. And yet, if a philosopher should offer eloquence, I would not spurn it; if he does not have it, I would not particularly demand it. It is in his content that he does not equally satisfy me, and indeed in several places. But, so many men, so many opinions; therefore, we can be mistaken." "Why then," he said, "does he not satisfy you? For I think you are a fair judge, provided you know well what he says."

"Unless," I said, "you think that Phaedrus or Zeno lied to me—both of whom I heard (though they impressed me with nothing, frankly, except their diligence)—all of Epicurus's doctrines are quite well known to me. And I frequently heard those men I named with our friend Atticus, while he [Atticus] admired both of them, and indeed was fond of Phaedrus. And every day we would discuss between us those things which we had heard, and there was never any dispute about what I understood, but only about what I approved."
"""

input_ids = tokenizer.encode(sentence, return_tensors="pt").to(model.device)[:, :4096]
seq_len = input_ids.shape[1]

model_inputs = {
    "input_ids": input_ids,
    "attention_mask": torch.tril(torch.ones(seq_len, seq_len, dtype=torch.bool, device=model.device)).unsqueeze(0).unsqueeze(0),
    # "full_hidden_states": True,
}

with torch.no_grad():
    hidden_states = model(**model_inputs).hidden_states[0].float().cpu().numpy()
    logits = model.lm_head(torch.tensor(hidden_states, device=model.device, dtype=torch.bfloat16).unsqueeze(0)).float()
    logits_softmax = torch.nn.functional.softmax(logits, dim=-1)
    input_ids_prob = logits_softmax.gather(2, input_ids.unsqueeze(-1)).squeeze(-1)[0].flatten().float().cpu().numpy()

model_inputs = {
    "input_ids": input_ids,
    "attention_mask": torch.tril(torch.ones(seq_len, seq_len, dtype=torch.bool, device=model.device)).unsqueeze(0).unsqueeze(0),
    # "full_hidden_states": True,
    "freq_threshold": 1,#(-10, -1),
    "lpf_layers": [25]
}

with torch.no_grad():
    hidden_states = model(**model_inputs).hidden_states[0].float().cpu().numpy()
    logits_lpf = model.lm_head(torch.tensor(hidden_states, device=model.device, dtype=torch.bfloat16).unsqueeze(0)).float()
    logits_lpf_softmax = torch.nn.functional.softmax(logits_lpf, dim=-1)
    input_ids_lpf_prob = logits_lpf_softmax.gather(2, input_ids.unsqueeze(-1)).squeeze(-1)[0].flatten().float().cpu().numpy()


# find the most changed token prob
prob_diff = input_ids_prob - input_ids_lpf_prob
most_changed_idx_decrease = np.argsort(prob_diff)[-50:][::-1]
most_changed_idx_increase = np.argsort(prob_diff)[:50]

print("Most increased token probs after LPF:")
for idx in most_changed_idx_increase:
    context = tokenizer.decode(input_ids[0, max(0, idx-10):min(input_ids.shape[1], idx+10)].cpu().numpy())
    new_most_prob_token = torch.argmax(logits_lpf[0, idx]).item()
    new_most_prob_token_str = tokenizer.decode(new_most_prob_token)
    new_most_prob_token_prob = logits_lpf_softmax[0, idx, new_most_prob_token].item()
    new_rank = torch.sum((logits_lpf_softmax[0, idx] > logits_lpf_softmax[0, idx, input_ids[0, idx]]).float()).item() + 1
    new_context = tokenizer.decode(input_ids[0, max(0, idx-10):idx].cpu().numpy().tolist() + [new_most_prob_token] + input_ids[0, idx+1:min(input_ids.shape[1], idx+10)].cpu().numpy().tolist())
    print(
        f"Token idx: {idx}, \n"
        f"Old Token: {tokenizer.decode(input_ids[0, idx].item())}, "
        f"Original Prob: {input_ids_prob[idx]:.6f}, "
        f"LPF Prob: {input_ids_lpf_prob[idx]:.6f}, "
        f"Diff: {prob_diff[idx]:.6f}, \n"
        f"Old Context: ...{context}..., \n"
        f"New Token: {new_most_prob_token_str}, "
        f"New Prob: {new_most_prob_token_prob:.6f}, "
        f"New Rank: {new_rank}, \n"
        f"New Context: ...{new_context}..."
        f"\n"
    )

print("Most decreased token probs after LPF:")
for idx in most_changed_idx_decrease:
    context = tokenizer.decode(input_ids[0, max(0, idx-10):min(input_ids.shape[1], idx+10)].cpu().numpy())
    new_most_prob_token = torch.argmax(logits_lpf[0, idx]).item()
    new_most_prob_token_str = tokenizer.decode(new_most_prob_token)
    new_most_prob_token_prob = logits_lpf_softmax[0, idx, new_most_prob_token].item()
    new_rank = torch.sum((logits_lpf_softmax[0, idx] > logits_lpf_softmax[0, idx, input_ids[0, idx]]).float()).item() + 1
    new_context = tokenizer.decode(input_ids[0, max(0, idx-10):idx].cpu().numpy().tolist() + [new_most_prob_token] + input_ids[0, idx+1:min(input_ids.shape[1], idx+10)].cpu().numpy().tolist())
    print(
        f"Token idx: {idx}, \n"
        f"Old Token: {tokenizer.decode(input_ids[0, idx].item())}, "
        f"Original Prob: {input_ids_prob[idx]:.6f}, "
        f"LPF Prob: {input_ids_lpf_prob[idx]:.6f}, "
        f"Diff: {prob_diff[idx]:.6f}, \n"
        f"Old Context: ...{context}..., \n"
        f"New Token: {new_most_prob_token_str}, "
        f"New Prob: {new_most_prob_token_prob:.6f}, "
        f"New Rank: {new_rank}, \n"
        f"New Context: ...{new_context}..."
        f"\n"
    )
